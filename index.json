[{"uri":"/01introduction.html","title":" 介绍","tags":[],"description":"","content":"概述 数字资产盘活机器人是一个将机器学习应用于业务场景的解决方案，客户可以使用此解决方案中的机器人，对其上传至Amazon Web Services （AWS）云上的数字资产（例如，照片、PDF文档、视频等非结构化数据）进行知识标记。标记知识将有助于客户实现业务流程自动化。\n此解决方案利用Amazon Simple Storage Service (Amazon S3) 和Amazon Elastic Compute Cloud (Amazon EC2) 的竞价型实例（Spot Instances），使客户可以安全的、低成本的使用批处理机器人来标记数字资产。机器人将所识别出的信息存储在客户私有的Amazon S3 存储桶中来保证数据安全性，机器人将运行在EC2竞价型实例中来节省计算成本。\n您可以访问 https://www.amazonaws.cn/solutions 查看最新版本的机器人。此解决方案第一批将发布三个机器人：基于图像识别技术的车型分类机器人、基于OCR技术的的场景文字识别机器人和基于自然语言处理技术的情感分析机器人。第二批计划发布视频相关的机器人，如配乐文本、目标角色识别、监控对象检测等。第三批计划自然语言处理相关的机器人，如关键词检测、年度报告阅读等。】\n此解决方案是一个开源框架，构建者可以通过创建自己的模型或机器人为本解决方案框架做出贡献。\n内建机器人 本解决方案内建三个机器人：\n 车型分类机器人：通过图像分类方法，将输入车辆图像进行分类，识别出汽车生产厂商、汽车型号等信息 情感分析机器人：通过调优预训练的中文BERT模型，将输入中文文本分类为正面/负面评论 身份证识别机器人：通过识别身份证中的文字，识别身份证中的人名、地址等信息  此外，用户可以通过API Gateway调用的方式添加/删除自定义机器人，从而达到扩展该解决方案的目的。\n"},{"uri":"/01introduction/100usecase.html","title":"使用场景","tags":[],"description":"","content":"使用场景 车型分类机器人 保险公司需要对交通事故进行定责时，可以利用此机器人对用户现场拍的车辆照片进行处理分析。机器人可以快速返回车辆类型、生产年限等信息。这样可提高保险理赔效率，提升用户体验。\n场景文字识别机器人 医疗保险公司对业务处理流程中的扫描文档和图片进行审核时，可以利用此机器人训练自己的模型并进行审核。这样可以节省过去采用人工审核及第三方服务的方式所产生的费用，并且提升准确率和响应速率。\n情感分析机器人 金融投资公司调研热点新闻对于股票市场的影响时，可以利用此机器人对历史财经新闻进行分析建模，并对热点新闻进行情感分析。这样可以更高校、更准确的获取市场反馈信息。\n"},{"uri":"/01introduction/200whyusespotbot.html","title":"数字化资产盘活机器人优点","tags":[],"description":"","content":"定制化能力： 带有基于容器的训练框架，可以导入自有的标注数据进行训练\n易用性： 服务提供API接口\n资源整合 用户仅需要将需要训练的数据上传到Amazon S3\n成本优化： 利用Amazon Spot Instance中的闲置资源进行训练\n本解决方案没有调用费用 仅需要支付该解决方案所使用的AWS相关服务的费用\n"},{"uri":"/02architect.html","title":"系统架构","tags":[],"description":"","content":"如何使用数字化资产盘活机器人解决方案 AWS[Steward] 解决方案包含一个AWS CloudFormation模版，帮助你在你的AWS账号中快速部署。这个模版会启动你部署本解决方案所需要的所有资源和权限设置。\n当模版部署后，你可以指定你所需要盘活的数据资产的S3路径及对应所需要启动的机器人。AWS[Steward] 解决方案支持很多的数据资产类型，包含文本，视频，图片等。指定的机器人在启动后会根据当前S3的内容生成一份任务列表，列表中包含了需要处理的数字资产，比如1000个需要打标签的视频。具体的任务会被分段（Batch），然后多个机器人实例会处理各自的任务段，如果一共有10个机器人，每个片段可能包括100个视频。\n这些机器人实例都是运行在Spot Instance上的，所以在计算上具有很高的性价比。机器人处理完当前任务后会将结果写入Elastic Search和S3。如果某个机器人实例发生异常，则相应的任务会交给其他的机器人实例来重做。\n客户可以通过Elastic Search查询任务的结果，并且了解整个任务的执行状态。\u000b每个机器人都是由两层构成的，一层实现的业务需求，比如OCR机器人可以完成OCR任务，但是具体的OCR任务可能需要多个模型协作完成，比如一个模型把文本部分框出来，另一个模型只负责识别框好的文字，这些模型是第二层，为业务层提供能力支持。\n"},{"uri":"/01introduction/300botype.html","title":"机器人类型","tags":[],"description":"","content":"机器人类型 本解决方案内建三个机器人：\n 场景文字识别机器人：可识别图片中的印刷体中文文本信息。该机器人使用两个模型来完成身份证识别任务。模型一是Connectionist Text Proposal Network（CTPN）模型，图片经过CTPN后可以取出文字位置的坐标信息，之后，包含文字的图片被送入第二个名为Convolutional Recurrent Neural Network（CRNN）的模型去识别图片中文字的内容。 车型分类机器人：通过图像分类方法，将输入车辆图像进行分类，识别出车辆生产厂商、车辆型号等信息。本模型基于AutoML技术，利用AutoGluon在ResNet50基础上生成新的对车辆类别进行分类的图像分类模型。 情感分析机器人：通过调优预训练的中文BERT模型，将输入中文文本分类为正面/负面评论。我们基于公开的评论数据集预训练了机器人背后的模型。  以上这些模型和示例的训练和部署代码都已经在开源的代码库中，您可以直接使用，也可以在这些模型的基础上用您自己的数据进行增量训练（Fine Tune），训练和部署需要用到Amazon SageMaker服务。\n1. OCR 机器人 Xxx公司拥有很多业务处理流程中的扫描文档和图片，过去是使用人工审核+第三方服务的方式，通过本解决方案，可以训练自己的模型，节省了大量人工审核的费用，提升了准确率和响应速率\n收益:\n业务部门\n对于简单病历，可以降低“骗保”的风险，和提高诊所和该保险公司的合作满意度； 对于负责病历，可以提升客户体验，降低等待理赔的时间。\n审核中心\n对于业务审核中心，提高初次识别的准确度，降低和和诊所或医院沟通成本，降低审核周期； 利用机器学习，积累识别的经验，将“人眼和人脑”的知识积累沉淀到“模型”； 未来可以从事后审核，逐步过渡到事中风险提示和事前预警。\nIT\n盘活“历史数据资产”，从静态索引，逐步可以提供按照业务审核的新需求去灵活的更新查询索引； 讲”历史数据资产“上云，降低历史负债。\n数字化和业务创新\n根据自己的业务场景和历史数据来训练和优化模型、同时可控数据安全性； 利用AWS SageMaker逐步建立自己的数据活化、分析、建模和应用的能力。\n2. 车型分类机器人 当汽车发生事故时，xxx保险公司需要定责，用户拍的事故现场照片，使用本机器人，可以快速的返回事故发生的车的类型，生产年限信息\n业务部门 传统的核保和理赔核损方法，都是人工在现场采集标的全方位信息，然后回传到公司，并由专人进行车辆情况的评估。这种方法服务效率低且成本高，而且人工操作不可避免的会有工作失误和徇私舞弊，保险公司也很难责任追究。 在核保环节，主要涉及到车身划痕识别和自然场景下的OCR识别。通过算法模型的建立以及车身图像数据对算法的训练优化，可以实现智能核保，提升效率。 理赔核损环节，通过图像识别技术，将后台的标的照片以部位维度进行智能分类，之后使用图像识别技术进行损伤程度的评估，并输出核损报告。\n数字化和业务创新 根据自己的业务场景和历史数据来训练和优化模型、同时可控数据安全性； 利用AWS SageMaker逐步建立自己的数据活化、分析、建模和应用的能力。\n3. 情感分类机器人 Xxx公司最近推出了新的金融产品，想要调研它的市场反响，通过对收集的用户评论进行情感分析，可以得到准确的市场反馈信息。\n业务部门 随着互联网的飞速进步和全球金融的高速发展，金融信息呈现爆炸式增长。如何从海量的金融文本中快速准确地挖掘出关键信息，成为了投资者和决策者重点考虑的问题之一。使用金融文本中的信息主体的挖掘和面向主体的负面消息检测，在风控和舆情分析等领域有很大现实意义。\n数字化和业务创新 根据自己的业务场景和历史数据来训练和优化模型、同时可控数据安全性； 利用AWS SageMaker逐步建立自己的数据活化、分析、建模和应用的能力。\n"},{"uri":"/03deployment.html","title":"系统部署","tags":[],"description":"","content":"部署前提  确保当前使用的 AWS 账户已经在国内通过 ICP 备案，并需确保API Gateway可以通过公网访问80及443端口。 检查您的SageMaker Host Instance Limit，其中的 endpoint/ml.m5.large机型有8个以上的空闲。具体请参考Amazon SageMaker 终端节点和配额 (https://docs.aws.amazon.com/zh_cn/general/latest/gr/sagemaker.html) 检查您的AWS Lambda函数中不包含名称为sam_spot_bot_create_job或sam_spot_bot_api_receiver的函数。 检查IAM中是否已经存在Amazon Elasticsearch Service的Service-Linked Role，如果该 Role 已存在，请删除该 Role。  检查是否存在 Amazon Elasticsearch Service的Service-Linked Role   登录 AWS Console (中国) 点击 https://console.amazonaws.cn/iam/home?region=cn-northwest-1#/roles\n  在搜索框输入 AWSserviceRoleForAmazonElasticsearchService，例如 如果已经存在 AWSserviceRoleForAmazonElasticsearchService Role,请删除。否则 Cloudformation 堆栈无法创建成功。\n  "},{"uri":"/04trainmodel.html","title":"模型训练","tags":[],"description":"","content":"本模块通过一系列常见使用案例的简单示例来演示 SageMaker 的主要功能。您将了解一些机器学习概念以及它们与 Amazon SageMaker 的关系，并为研讨会创建 SageMaker 笔记本实例。\n"},{"uri":"/05usebot.html","title":"使用机器人","tags":[],"description":"","content":"使用 cloudformation 创建 EC2 并创建 S3 Bucket 为了方便大家测试 SpotBot 的功能，本次 workshop 提供一个 cloudformation 帮助大家快速的创建一个 S3 存储桶用于存储 3 个机器人使用的样例文件及存储识别结果。同时也会创建一台位于公网的 EC2 (Amazon Linux2),用户大家调用 API Gateway 及 Elasticsearch。\n点击如下的链接\n\nhttps://aws-solutions-reference.s3.cn-north-1.amazonaws.com.cn/spot-bot/v1.0.0/spot-bot-bastion-ec2-china.yaml 如下图所示 点击 【下一步】按钮 确保 PublicSubnet1 选择 spot-bot-PublicSunbet2, VPC 选择 SpotBot 点击 【下一步】按钮, 保持默认值，点击 【下一步】按钮, 确保勾选 “我确认，AWS CloudFormation 可能创建具有自定义名称的 IAM 资源。” checkbox。 点击 【创建堆栈】按钮。 等待大约 3 分钟，创建创建成功。 资源列表如下： 请点击 【输出】标签页 新建 S3 存储桶 ，命名规则为 spot-bot-exampledata-regionname-accountid. 进入 S3 console, 示例文件及目录如图 点击 【资源】 标签页，记录 EIP。例如下图中的 EIP 字段 52.82.89.118 详细目录结果如下 示例文件目录包含 3 个目录：/car_bot, /ocr_bot, /sentiment_bot。分别对应于车型分类机器人，场景文字识别机器人，情感分析机器人。每个机器人的目录中分别存在 /input 及 /output 目录，用于存储示例输入文件及存储示例文件的推理结果。 具体目录结构如下：\n├── car_bot │ ├── input │ │ ├── 2020-Audi-TTS-Coupe-Orange.png │ │ ├── benz.jpg │ │ ├── porsche_mission_e_cross_turismo_2018.jpg │ │ └── rh_ferraripistauk-31.jpg │ └── output │ ├── porsche_mission_e_cross_turismo_2018.json │ └── rh_ferraripistauk-31.json ├── ocr_bot │ ├── input │ │ ├── ocr_1.png │ │ ├── ocr_2.png │ │ └── ocr_3.png │ └── output │ ├── ocr_1.json │ ├── ocr_2.json │ └── ocr_3.json ├── sentiment_bot │ ├── input │ │ ├── example_n_1.txt │ │ ├── example_p_1.txt │ │ ├── example_p_2.txt │ │ └── example_p_3.txt │ └── output │ ├── example_n_1.json │ ├── example_p_1.json │ ├── example_p_2.json │ └── example_p_3.json 堡垒机的相关信息  由于本解决方案为了安全，从系统架构上无法从 Internet 上直接访问 Elasticsearch，所以特开启一台具有 EIP 的 EC2 作为堡垒机方便大家使用. 通过如下的方式获得堡垒机的 EIP. 点击 【资源】 标签页，记录 EIP。例如下图中的 EIP 字段 52.82.89.118   "},{"uri":"/06deepdivebot.html","title":"DeepDive机器人","tags":[],"description":"","content":"深入理解机器人 机器人在软件上的形式是Docker容器，它们存储在Amazon ECR上，每个机器人都包括下述两个部分：\n 业务逻辑处理模块。该模块主要负责处理Amazon S3的输入输出，负责调用推理终端节点及其它业务逻辑。 推理终端节点。此解决方案自带预训练的模型，预训练模型在Amazon SageMaker中启动后即可用于推理，并可被相应的业务处理模块调用。  架构与工作原理简介 ** 任务生成 **\n首先，要将待盘活的数字资产存储在Amazon S3上。然后，您只需要向Amazon API Gateway发出REST请求，指定要处理的Amazon S3文件夹和要使用的机器人类型即可。机器人具体运行流程如下：\n Amazon API Gateway将请求转发到AWS Lambda。 AWS Lambda 递归取出Amazon S3中所有待处理的文件路径，并生成待处理文件列表。 AWS Lambda 将待处理文件列表存入Amazon Elasticsearch Service中。 AWS Lambda 启动Amazon SageMaker的推理终端节点。 AWS Lambda 通过AWS Step Functions启动一个或者多个AWS Batch任务。 AWS Batch 执行相应资产盘活任务，从 Amazon S3 存储桶中读取源文件，并调用 Sagemaker的终端节点进行推理。推理结果会被写入 Amazon Elasticsearch Service (https://amazonaws-china.com/cn/elasticsearch-service/) 的索引中，识别结果将被写入您指定的S3路径。  系统流程图 服务交互  红线表示Planner和Bot的集成点，蓝线表示Bot需要实现的部分。 深黄色是需要预先provison的service/code。 Bot部分的docker image是可以代码创建出来的。  "},{"uri":"/07cleanup.html","title":"资源清理","tags":[],"description":"","content":"资源清理 数字资产盘活机器人中使用的服务绝大多数都是On Demand的类型，它们仅仅在运行时启动并产生费用。只有推理终端节点在使用后会一直运行，如果您不需要频繁启动数字资产盘活任务，您也可以发送REST请求或从控制台中关闭推理终端节点。\n"},{"uri":"/03deployment/0302verify/100apigateway.html","title":"APIGateway","tags":[],"description":"","content":"验证 API Gateway 已经成功创建 假设您在宁夏Region 创建资源，请点击 https://cn-northwest-1.console.amazonaws.cn/apigateway/home?region=cn-northwest-1#\n点击上图的链接 spot-bot 显示 API Gateway 的相关信息，说明 API Gateway 已经成功创建。\n"},{"uri":"/03deployment/0302verify/200lambda.html","title":"Lambda","tags":[],"description":"","content":"检查 Lambda 是否被成功创建 假设您在宁夏Region 创建资源， 点击 https://cn-northwest-1.console.amazonaws.cn/lambda/home?region=cn-northwest-1#/functions 本解决方案的 Lambda 函数 sam_spot_bot_api_reciever 及 sam_spot_bot_create_job 已成功创建。\nLambda 函数 sam_spot_bot_api_reciever 该函数负责接受用户提交的作业定义（JSON格式)。作业成功接收后，会返回 JSON 字符串。\nLambda 函数 sam_spot_bot_create_job 该函数负责创建数字资产盘活任务。\n Amazon API Gateway将请求转发到AWS Lambda。 AWS Lambda 递归取出Amazon S3中所有待处理的文件路径，并生成待处理文件列表。 AWS Lambda 将待处理文件列表存入Amazon Elasticsearch Service中。 AWS Lambda 启动Amazon SageMaker的推理终端节点。 AWS Lambda 通过AWS Step Functions启动一个或者多个AWS Batch任务。 AWS Batch 执行相应资产盘活任务，从 Amazon S3 存储桶中读取源文件，并调用 Sagemaker的终端节点进行推理。推理结果会被写入 Amazon Elasticsearch Service (https://amazonaws-china.com/cn/elasticsearch-service/) 的索引中，识别结果将被写入您指定的S3路径。  "},{"uri":"/02architect/0201overview.html","title":"架构概览","tags":[],"description":"","content":"架构图 架构与工作原理简介 使用流程  运营人员通过REST API指定S3文件夹和要执行的任务类型，及配置的运行机器的资源类型，如OCR，s3://a-media-bucket/images，3 instances Execution Planner接收到指令后去S3爬取全部待处理文件的元数据并存放在DynamoDB中，随后以AWS Batch Job的形式启动相应的机器人执行相应的任务。 机器人将调用预先部署好的模型推理接口，将任务的结果存放在转移到S3  任务生成 首先，要将待盘活的数字资产存储在Amazon S3上。然后，您只需要向Amazon API Gateway发出REST请求，指定要处理的Amazon S3文件夹和要使用的机器人类型即可。机器人具体运行流程如下：\n Amazon API Gateway将请求转发到AWS Lambda。 AWS Lambda 递归取出Amazon S3中所有待处理的文件路径，并生成待处理文件列表。 AWS Lambda 将待处理文件列表存入Amazon Elasticsearch Service中。 AWS Lambda 启动Amazon SageMaker的推理终端节点。 AWS Lambda 通过AWS Step Functions启动一个或者多个AWS Batch任务。 AWS Batch 执行相应资产盘活任务，从 Amazon S3 存储桶中读取源文件，并调用 Sagemaker的终端节点进行推理。推理结果会被写入 Amazon Elasticsearch Service (https://amazonaws-china.com/cn/elasticsearch-service/) 的索引中，识别结果将被写入您指定的S3路径。  "},{"uri":"/02architect/0210component.html","title":" 组件","tags":[],"description":"","content":"系统部署图 组件 此快速入门使用的核心 AWS 组件包括以下 AWS 服务。如果您是初次使用 AWS，请参阅 AWS 文档的入门部分。\n S3 (https://amazonaws-china.com/cn/s3/) - Amazon Simple Storage Service (Amazon S3) 是一种对象存储服务，提供行业领先的可扩展性、数据可用性、安全性和性能。这意味着各种规模和行业的客户都可以使用它来存储和保护各种用例（如网站、移动应用程序、备份和还原、存档、企业应用程序、IoT 设备和大数据分析）的任意数量的数据。 IAM (https://amazonaws-china.com/cn/iam/) - AWS Identity and Access Management (IAM) 使您能够安全地管理对 AWS 服务和资源的访问。您可以使用 IAM 创建和管理 AWS 用户和组，并使用各种权限来允许或拒绝他们对 AWS 资源的访问。 API Gateway (https://amazonaws-china.com/cn/api-gateway/) - Amazon API Gateway 是一种完全托管的服务，可以帮助开发人员轻松创建、发布、维护、监控和保护任意规模的 API。API 充当应用程序的前门，可从您的后端服务访问数据、业务逻辑或功能。使用 API Gateway，您可以创建 RESTful API 和 WebSocket API，以便实现实时双向通信应用程序。API Gateway 支持容器化和无服务器工作负载，以及 Web 应用程序。 Lambda (https://amazonaws-china.com/cn/lambda/) - 借助 Lambda，您几乎可以为任何类型的应用程序或后端服务运行代码，而且完全无需管理。只需上传您的代码，Lambda 会处理运行和扩展高可用性代码所需的一切工作。您可以将您的代码设置为自动从其他 AWS 服务触发，或者直接从任何 Web 或移动应用程序调用。 StepFunctions (https://amazonaws-china.com/cn/step-functions/) - AWS Step Functions 是一个无服务器函数编排工具，可轻松将 AWS Lambda 函数和多个 AWS 服务按顺序安排到业务关键型应用程序中。通过其可视界面，您可以创建并运行一系列检查点和事件驱动的工作流，以维护应用程序状态。每一步的输出作为下一步的输入。应用程序中的各个步骤根据您定义的业务逻辑按既定顺序执行。 Elasticsearch Service (https://amazonaws-china.com/cn/elasticsearch-service/) - Amazon Elasticsearch Service 是一项完全托管的服务，方便您大规模经济高效地部署、保护和运行 Elasticsearch。您可以按照您需要的规模、使用喜欢的工具构建和监控应用程序并解决其中的问题。该服务提供开放源 Elasticsearch API、托管 Kibana (https://amazonaws-china.com/cn/elasticsearch-service/the-elk-stack/kibana/)、与 Logstash (https://amazonaws-china.com/cn/elasticsearch-service/the-elk-stack/logstash/) 和其他 AWS 服务的集成以及内置提醒和 SQL 查询支持。使用 Amazon Elasticsearch Service 时，您只需按实际用量付费，没有预付成本或使用要求。使用 Amazon Elasticsearch Service，您无需承担运营开销，便可获得所需的 ELK 堆栈。 S3 终端节点 (https://docs.aws.amazon.com/zh_cn/vpc/latest/userguide/vpc-endpoints-s3.html) - S3终端节点是一个种VPC终端节点。VPC 终端节点 使您能够将 VPC 私密地连接到支持的 AWS 服务和 VPC 终端节点服务（由 AWS PrivateLink 提供支持），而无需互联网网关、NAT 设备、VPN 连接或 AWS Direct Connect 连接。VPC 中的实例无需公有 IP 地址便可与服务中的资源通信。VPC 和其他服务之间的通信不会离开 Amazon 网络。 AWS Batch (https://amazonaws-china.com/cn/batch/) - AWS Batch 让开发人员、科学家和工程师能够轻松高效地在 AWS 上运行成千上万个批处理计算作业。AWS Batch 可根据提交的批处理作业的卷和特定资源需求动态预置最佳的计算资源（如 CPU 或内存优化实例）数量和类型。借助 AWS Batch，您无需安装和管理运行您的作业所使用的批处理计算软件或服务器集群，从而使您能够专注于分析结果和解决问题。AWS Batch 将通过全系列的 AWS 计算服务和功能（如 Amazon EC2 (https://amazonaws-china.com/cn/ec2/) 和 Spot 实例 (https://amazonaws-china.com/cn/ec2/spot/)）计划、安排和执行批量计算工作负载。 SageMaker (https://amazonaws-china.com/cn/sagemaker/) - Amazon SageMaker 是一项完全托管的服务，可以帮助开发人员和数据科学家快速构建、训练和部署机器学习 (ML) 模型。SageMaker 完全消除了机器学习过程中每个步骤的繁重工作，让开发高质量模型变得更加轻松。 ECR (https://amazonaws-china.com/cn/ecr/) - Amazon Elastic Container Registry (ECR) 是完全托管的 Docker (https://amazonaws-china.com/cn/docker/) 容器注册表，可使开发人员轻松存储、管理和部署 Docker 容器映像。Amazon ECR 与 Amazon Elastic Container Service (ECS) (https://amazonaws-china.com/cn/ecs/) 集成，从而简化生产工作流程的开发。Amazon ECR 使您无需操作自己的容器注册表，或使您不必为扩展底层基础架构而感到担心。  "},{"uri":"/03deployment/0301cloudformation.html","title":"Cloudformation 部署","tags":[],"description":"","content":"使用 Cloudformation 部署 部署前提条件\n 确保当前使用的 AWS 账户已经在国内通过 ICP 备案，并需确保Amazon API Gateway可以通过公网访问80及443端口。 检查您的SageMaker Host Instance Limit，其中的 endpoint/ml.m5.large机型有8个以上的空闲。具体请参考Amazon SageMaker 终端节点和配额 (https://docs.aws.amazon.com/zh_cn/general/latest/gr/sagemaker.html) 确保您的AWS Lambda中不包含名称为sam_spot_bot_create_job或sam_spot_bot_api_receiver的函数。 检查IAM中是否已经存在Amazon Elasticsearch Service的Service-Linked Role，如果该 Role 已存在，请删除该 Role。  CloudFormation 文件 Spotbot solution 使用嵌套堆栈方式创建资源，共有两个 Cloudformation 文件。\n spot-bot-vpc-china.template - 创建 VPC 及 S3 endpoint spot-bot-china.template - 创建 API Gateway, Lambda, Stepfunction, Batch 等资源  使用 CloudFormation 部署 Spotbot 步骤1: 启动CloudFormation堆栈\n此自动化 AWS CloudFormation 模板在 AWS 账户中部署 数字资产盘活机器人 应用程序。 您负责运行此解决方案时使用的AWS服务的成本。 有关更多详细信息，请参见“费用”部分。 有关完整详细信息，请参阅此解决方案中将使用的每个AWS服务的定价页面。\n 登录到AWS管理控制台，然后单击下面的按钮以启动 AWS CloudFormation 模板。点击如下链接  \n 默认情况下，该模板将在 AWS （宁夏）区域启动。 若需在其他AWS区域中启动该解决方案，请使用控制台导航栏中的区域选择器。\n  在创建堆栈页面上，确认 Amazon S3 URL 文本框中显示正确的模板URL，然后选择 下一步。   在指定堆栈详细信息下，需要指定如下参数。\n AvailabilityZones = [“cn-northwest-1a”, “cn-northwest-1b”]；注意：AvailabilityZones需要选择两个AZ , 点击 【下一步】    在 配置堆栈选项 页面上，保持默认值，点击 【下一步】   在 审核 Spot-bot 页面，确保勾选如下两个选项 我确认，AWS CloudFormation 可能创建具有自定义名称的 IAM 资源。 我确认，AWS CloudFormation 可能需要以下功能: CAPABILITY_AUTO_EXPAND ,点击 【创建堆栈】。\n  等待大概20 分钟后，堆栈创建成功。可以点击 【输出】标签页，显示详细信息。   "},{"uri":"/03deployment/0302verify.html","title":"环境验证","tags":[],"description":"","content":"需要验证的服务 成功运行 Cloudformation 后，将会创建如下关键资源。\n APIGateway Lambda Elasticsearch  随后我们会对这些资源进行一一的检查验证。\n"},{"uri":"/03deployment/0302verify/500elasticsearch.html","title":"Elasticsearch","tags":[],"description":"","content":"检查 Elastic Search 是否创建成功  登录 Elastic Search Console https://cn-northwest-1.console.amazonaws.cn/es/home?region=cn-northwest-1 检查 Spot-bot Domain 是否在 ElasticSearch 中创建成功  点击上图的 Spot-Bot 链接，显示当前 ElasticSearch Domain 详情。   "},{"uri":"/04trainmodel/0401setup/040101jupyter.html","title":"创建SageMaker笔记本实例","tags":[],"description":"","content":"创建笔记本实例 SageMaker 提供无需设置的托管Jupyter Notebook，因此您可以立即开始处理您的训练数据集。只需在 SageMaker 控制台中单击几下，您就可以创建完全托管的笔记本实例，预先加载了用于机器学习的有用库。您只需添加您的数据。\n首先，您将创建将在整个研讨会中使用的 Amazon S3 存储桶。然后，您将创建一个 SageMaker 笔记本实例，该实例将用于其他研讨会模块。\n创建 S3 存储桶 SageMaker 通常使用 S3 作为数据和模型工件的存储。在此步骤中，您将为此目的创建 S3 存储桶。要开始，请登录 AWS 管理控制台 https://console.amazonaws.cn/。\n请记住，您的存储桶名称必须在所有区域和客户中具有全球唯一性。我们建议使用像 smworkshop-firstname-lastname 这样的名字。如果您收到存储桶名称已存在的错误信息，请尝试添加其他数字或字符，直到找到未使用的名称。\n  在 AWS 管理控制台中，选择服务，然后在存储下选择 S3。\n  选择创建存储桶\n  为您的存储桶提供全局唯一的名称，例如 “smworkshop-firstname-lastname”。\n  从下拉列表中选择您选择用于此研讨会的区域。\n  在对话框左下角选择创建，而不选择要从中复制设置的存储桶。\n  启动笔记本实例   在 AWS 管理控制台的右上角，确认您位于所需的 AWS 区域。选择由西云数据运营的AWS(宁夏)区域或由光环新网运营的AWS(北京)区域。\n  点击所有服务列表中的亚马逊 SageMaker。这将带您访问亚马逊 SageMaker 控制台主页。\n  要创建新的笔记本实例，请转到笔记本实例，然后单击浏览器窗口顶部的创建笔记本实例按钮。  在笔记本实例名称文本框中键入 smworkshop-[名]-[姓氏]，然后选择 ml.m4.xlarge 作为笔记本实例类型。  对于 IAM 角色，选择创建新角色，然后在生成的弹出模式中，选择您指定的 S3 存储桶下的任意 S3 存储桶。单击创建角色。   在IAM中选择角色，在角色列表中选择刚刚创建的SageMaker-ExecutionRole，并添加AmazonEC2ContainerRegistryFullAccess Policy。\n  您将返回 “创建笔记本” 实例页面。单击创建笔记本实例。\n  访问笔记本电脑实例  等待服务器状态更改为 InService。这将需要几分钟，可能最多 10 分钟，但可能更少。  单击 “打开”。您现在将看到您的笔记本实例的 Jupyter 主页。  "},{"uri":"/04trainmodel/0401setup.html","title":"环境搭建","tags":[],"description":"","content":"环境 AWS 账户 要完成本次研讨会，您需要一个 AWS 账户，以及该账户中的一个 AWS IAM 用户至少具有以下 AWS 服务的完全权限：\n AWS IAM Amazon S3 Amazon SageMaker  使用您自己的账户：本研讨会中的代码和说明假定一次只有一名学生正在使用给定的 AWS 账户。如果您尝试与其他学生共享帐户，您将遇到某些资源的命名冲突。您可以通过为由于冲突而无法创建的资源附加唯一后缀来解决这些问题，但说明不提供有关使此工作所需的更改的详细信息。为此研讨会使用个人账户或创建新的 AWS 账户，而不是使用组织的账户，以确保您能够完全访问必要的服务，并确保您不会留下研讨会的任何资源。\n成本：如果您的账户不到 12 个月，您将在此研讨会中启动的部分资源（但不是全部）有资格享受 AWS 免费套餐。有关更多详细信息，请参阅 AWS 免费套餐页面。免费套餐不覆盖的资源示例是某些研讨会中使用的 ml.m4.xlarge 笔记本实例。为了避免在完成研讨会后对终端节点和其他资源产生费用，请参阅清理模块。\nAWS 区域 目前，SageMaker 并非在所有 AWS 区域都可用。因此，我们建议在以下受支持的 AWS 区域之一举行此研讨会：\n 由西云数据运营的AWS(宁夏)区域 由光环新网运营的AWS(北京)区域  选择区域后，您应该为此研讨会创建所有资源，包括一个新的 Amazon S3 存储桶和一个新的 SageMaker 笔记本实例。在开始之前，请确保从 AWS 控制台右上角的下拉列表中选择您的区域。\n浏览器 我们建议您使用最新版本的 Chrome 或火狐浏览器来完成本次研讨会。\nAWS 命令行界面 要完成某些研讨会模块，您需要 AWS 命令行界面 (CLI) 和 Bash 环境。您将使用 AWS CLI 与 SageMaker 和其他 AWS 服务进行接口。\n文本编辑器 对于任何需要使用 AWS 命令行界面的研讨会模块（见上文），您还需要一个纯文本编辑器来编写 Bash 脚本。任何插入 Windows 或其他特殊字符的编辑器都可能导致脚本失败。\n"},{"uri":"/04trainmodel/0402carbot.html","title":"车险理赔机器人模型训练","tags":[],"description":"","content":"这里，我们来介绍一下如何使用sagemaker训练一个图片分类模型。共分为以下几步：\n 建立笔记本 构建训练任务 查看结果 模型部署  建立笔记本 在本教程中，我们将使用图像分类任务来说明如何使用 AutoGluon 的 API。\n我们将图像和相应的标签加载到 AutoGluon 中，并使用这些数据获取可对新图像进行分类的神经网络。这与传统的机器学习不同，我们需要手动定义神经网络，然后在训练过程中指定超参数。相反，只需对 AutoGluon 的拟合函数fit进行一次调用，AutoGluon 就会自动训练许多具有不同超参数配置的模型，并返回实现最高精度的模型。\n下载数据集\n在本教程中，我们将使汽车图像数据集, 该数据集包含 196 类汽车的 16,185 张图像。这些数据分为 8,144 张训练图像和 8,041 张测试图像，其中每个班级大致被分割为 50-50 分。类别通常是在品牌、模型、年份等级，例如 2012 年特斯拉 S 型或 2012 年宝马 M3 轿跑车。\n!wget http://imagenet.stanford.edu/internal/car196/cars_train.tgz !wget http://imagenet.stanford.edu/internal/car196/cars_test.tgz !wget http://ai.stanford.edu/~jkrause/cars/car_devkit.tgz 数据格式转换\n解压缩数据集\nif not os.path.exists(\u0026#34;devkit\u0026#34;): !tar xf car_devkit.tgz if not os.path.exists(\u0026#34;cars_train\u0026#34;): !tar xf cars_train.tgz if not os.path.exists(\u0026#34;cars_test\u0026#34;): !tar xf cars_test.tgz 读取标注数据\nimport scipy.io cars_meta = scipy.io.loadmat(\u0026#39;devkit/cars_meta.mat\u0026#39;) 转换数据格式\nmeta = [m[0] for m in cars_meta[\u0026#39;class_names\u0026#39;][0]] for m in meta: dirname = \u0026#34;data/train/{}\u0026#34;.format(m) if not os.path.exists(dirname): os.mkdir(dirname) dirname = \u0026#34;data/test/{}\u0026#34;.format(m) if not os.path.exists(dirname): os.mkdir(dirname) 可视化数据中的样本\nimport cv2 import matplotlib.pyplot as plt img = cv2.imread(\u0026#34;data/train/Jeep_Patriot_SUV_2012/00086.jpg\u0026#34;) img[:, :, (0, 1, 2)] = img[:, :, (2, 1, 0)] ax = plt.imshow(img) 构建训练任务 安装AutoGluon工具包\n!pip install -i https://pypi.tuna.tsinghua.edu.cn/simple autogluon==0.0.12 -t package 开始使用AutoGluon工具包，并定义任务\nimport autogluon as ag from autogluon import ImageClassification as task dataset = task.Dataset(\u0026#39;data/train\u0026#39;) classifier = task.fit(dataset, net=\u0026#39;ResNet50_v1b\u0026#39;, batch_size=64, epochs=20, ngpus_per_trial=1, num_trials=1, verbose=True) if not os.path.exists(\u0026#34;models\u0026#34;): os.makedirs(\u0026#34;models\u0026#34;) classifier.save(\u0026#34;models/resnet50_cars-0000.ag\u0026#34;) 查看结果 print(\u0026#39;Top-1 val acc: %.3f\u0026#39; % classifier.results[\u0026#39;best_reward\u0026#39;]) \u0026#34;Top-1 val acc: 0.724\u0026#34; image_path = \u0026#39;cars_test/00003.jpg\u0026#39; img = cv2.imread(image_path) clsidx, prob, _ = classifier.predict(image_path) print(clsidx.asscalar(), prob.asscalar(), classes[clsidx.asscalar()]) img[:, :, (0, 1, 2)] = img[:, :, (2, 1, 0)] ax = plt.imshow(img) 模型部署 import sagemaker from sagemaker import get_execution_role, local, Model, utils, fw_utils, s3 session = sagemaker.Session() local_session = local.LocalSession() bucket = session.default_bucket() prefix = \u0026#39;sagemaker/autogluon-image-classification\u0026#39; region = session.boto_region_name role = get_execution_role() client = session.boto_session.client( \u0026#34;sts\u0026#34;, region_name=region, endpoint_url=utils.sts_regional_endpoint(region) ) account = client.get_caller_identity()[\u0026#39;Account\u0026#39;] ecr_uri_prefix = utils.get_ecr_image_uri_prefix(account, region) registry_id = fw_utils._registry_id(region, \u0026#39;mxnet\u0026#39;, \u0026#39;py3\u0026#39;, account, \u0026#39;1.6.0\u0026#39;) registry_uri = utils.get_ecr_image_uri_prefix(registry_id, region) inference_algorithm_name = \u0026#39;autogluon-sagemaker-inference\u0026#39; algorithm_name = inference_algorithm_name fullname=f\u0026#34;{ecr_uri_prefix}/{algorithm_name}:latest\u0026#34; sagemaker_model = MXNetModel(model_data=model_data, image=fullname, role=role, sagemaker_session=session, py_version=\u0026#39;py3\u0026#39;, entry_point=\u0026#39;docker/serve.py\u0026#39;, framework_version=\u0026#39;1.6.0\u0026#39;) predictor = sagemaker_model.deploy(initial_instance_count=1, instance_type=\u0026#39;ml.m5.xlarge\u0026#39;) "},{"uri":"/04trainmodel/0403semanticbot.html","title":" 情感分类机器人模型训练","tags":[],"description":"","content":"这里，我们来介绍一下如何使用sagemaker训练一个情感分类模型。共分为以下几步：\n 下载数据 本地训练 SageMaker训练，部署  构建ecr镜像 模型训练 查看结果 模型部署   测试调用  下载数据 #download data/model files/JupyterNotebook wget https://spot-bot-asset.s3.amazonaws.com/spot-workshop-2020/demo2.tar.gz #untar tar -zxvf demo2.tar.gz 运行后，你可以看到对应的文件目录\n-|--bert |--data |--build_and_push.sh |--DockerFile |--train.ipynb 本地训练测试 source activate tensorflow_p36 export BERT_BASE_DIR=./bert/pretrain_model/chinese_L-12_H-768_A-12 python bert/run_classifier.py \\  --data_dir=\u0026#39;./data\u0026#39; \\  --task_name=\u0026#39;chnsenticorp\u0026#39; \\  --vocab_file=$BERT_BASE_DIR/vocab.txt \\  --bert_config_file=$BERT_BASE_DIR/bert_config.json \\  --output_dir=./output/ \\  --do_train=true \\  --do_eval=true \\  --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \\  --max_seq_length=200 \\  --train_batch_size=16 \\  --learning_rate=5e-5\\  --num_train_epochs=1.0\\  --save_checkpoints_steps=100 构建ecr镜像 这里，您可以自己构建也可以使用我们预置好的公共ECR镜像\n#build yourself sh build_an_push.sh sentiment-bot 预置好的公共ECR镜像的路径 \u0026quot;753680513547.dkr.ecr.\u0026quot; + self.region + \u0026quot;.amazonaws.com.cn/sentiment-analyisis-endpoint:latest\u0026quot;\n模型训练 打开train.ipynb文件，可以进行接下来的训练部署\n查看结果 建立训练任务\n查看训练监控\n模型产生的结果目录中，可以看到生成了eval_results.txt, 可以看到模型在测试集上的表现\n   指标 结果     eval_accuracy 0.93416667   eval_loss 0.31285414   recall 0.91399664   loss 0.31285414   auc 0.9339341   precision 0.9508772    模型部署 predictor = estimator.deploy(1, instance_type=\u0026#39;ml.m5.large\u0026#39;, endpoint_name=\u0026#39;bert-sentiment\u0026#39;) 运行后，会看到生成了对应的endpoint\n测试调用 "},{"uri":"/05usebot/0501ocrbot.html","title":"场景文字识别机器人","tags":[],"description":"","content":"场景文字识别机器人 可识别图片中的印刷体中文文本信息。该机器人使用两个模型来完成身份证识别任务。模型一是Connectionist Text Proposal Network（CTPN）模型，图片经过CTPN后可以取出文字位置的坐标信息，之后，包含文字的图片被送入第二个名为Convolutional Recurrent Neural Network（CRNN）的模型去识别图片中文字的内容。\n示例文件  场景文字识别的机器人的输入文件是中文文档的图片。 支持的文件格式是 jpg 或者 png 示例文件内容如下   "},{"uri":"/05usebot/0502carbot.html","title":" 车型识别机器人","tags":[],"description":"","content":"车型分类机器人 通过图像分类方法，将输入车辆图像进行分类，识别出车辆生产厂商、车辆型号等信息。本模型基于AutoML技术，利用AutoGluon在ResNet50基础上生成新的对车辆类别进行分类的图像分类模型。\n"},{"uri":"/05usebot/0503semanticbot.html","title":" 情感分析机器人","tags":[],"description":"","content":"情感分析机器人 通过调优预训练的中文BERT模型，将输入中文文本分类为正面/负面评论。我们基于公开的评论数据集预训练了机器人背后的模型。\n在测试数据集共有 4 个测试文件，分别是从京东和淘宝的评论区随机抓取\n测试文件 1 内容: 喝起来就是糖精+维他柠檬茶啊！严重怀疑茶底是用维他柠檬茶加糖精调好的，所以无法调糖，甜到爆炸，喝了几口受不了直接丢了 出品很一般……希望不要因为控制成本而影响出品，这样得不偿失\n测试文件 2 内容: 这个打印机还是很不错的。大小很好，适合家庭没什么地方放打印机的。而且打印的速度特别快，基本上我在打印之后不到半分钟吧，内容就已经可以打出来了，很快的，很适合家里有小朋友给孩子打作业呀习题呀。用了几天了，目前各方面都很满意，希望可以像宣传的一样，一盒墨可以打很多吧。\n测试文件 3 内容: 物流速度快，产品外观漂亮，投影效果很好，音质效果很好，操作简单。点赞，推荐给大家。\n测试文件 4 内容: 宝贝收到了，包装很好。画面清晰。音质效果好，投影亮度很柔和。外观设计大方，很实用。操作简单，和厂家描述的一致，物流也很快。推荐给朋友！\n期望测试结果  测试文件 1 - Negative 测试文件 2 - Positive 测试文件 3 - Positive 测试文件 4 - Positive  "},{"uri":"/05usebot/0504byob.html","title":"Bring Your Own Bot","tags":[],"description":"","content":"Bring Your Own Bot 由于模型层和任务执行层充分解耦合，您可以很方便的添加您自己的Bot到数字资产盘活机器人框架中，整体流程如下。\n准备模型 在SageMaker中训练模型，并启动一个可供调用的Endpoint 可以参考notebook导出的train.py，也可以使用SageMaker中自带的例子。\n 注意：执行框架如果检测到Endpoint不存在则会根据配置中指定的ECR来创建Endpoint，如果检测到bot配置中指定的Endpoint已经存在就不会再自动创建Endpoint。\n 准备机器人 机器人将从执行框架接收所需的输入，调用Endpoint，并将输出存在S3。\n步骤一：准备task.py task.py负责业务逻辑，可以参考代码中的task.py。这个文件的主要可复用逻辑如下：\n... # 配置和S3文件列表信息存在ES中，需要操作ES的Package from elasticsearch import Elasticsearch, RequestsHttpConnection ... # 执行框架传进来的参数 region_name = os.getenv(\u0026quot;region_name\u0026quot;) endpoint_name = os.getenv(\u0026quot;endpoint_name\u0026quot;) output_s3_bucket = os.getenv(\u0026quot;output_s3_bucket\u0026quot;) output_s3_prefix = os.getenv(\u0026quot;output_s3_prefix\u0026quot;) elastic_search_host = os.getenv(\u0026quot;es_host\u0026quot;) elastic_search_port = os.getenv(\u0026quot;es_port\u0026quot;) elastic_search_protocol = os.getenv(\u0026quot;es_protocol\u0026quot;) batch_id = os.getenv(\u0026quot;batch_id\u0026quot;) job_id = os.getenv(\u0026quot;job_id\u0026quot;) elastic_search_index = os.getenv(\u0026quot;es_index\u0026quot;） ... # 获取待处理文件列表 def __search_for_file_list(job_id: str, batch_id: str, status=\u0026quot;NOT_STARTED\u0026quot;) -\u0026gt; list: ... # 调用Endpoint的代码 def invoke_endpoint(session, endpoint_name, text): ... # 一个文件处理完后更新它的状态 def update_status_by_id(es, doc_id, status=\u0026quot;COMPLETED\u0026quot;, output=\u0026quot;\u0026quot;): ... 步骤二：将业务逻辑包装到bot镜像中并将它们部署到ECR 参考代码中的Dockerfile和build_and_push.sh，根据需要您可能会修改基础镜像，镜像名称等。\n注册新机器人 使用创建在EC2中的byob.py向执行框架添加新的机器人程序配置 Cloudformation在生成EC2时用下述指令下载了需要的脚本。\ncurl --location -o /home/ec2-user/byob.zip https://junxiang-solutions-cn-northwest-1.s3.cn-northwest-1.amazonaws.com.cn/byob/byob.zip unzip /home/ec2-user/byob.zip 下述脚本将bot_config.json中的配置更新到Elasticsearch中，如果Bot配置不存在会创建新的配置，如果存在则会更新。\ncd ~/byob # Step 1. Install Elasticsearch package. pip3 -r requirement.txt # Step 2. Define the bot config. vi bot_config.json # Step 3. Create or update a bot. python3 byob.py bot_config.json的参数和使用方法 { \u0026#34;es_url\u0026#34;: \u0026#34;https://vpc-spot-bot-xqra2d6vha4waz4oer2yqx2vfu.cn-northwest-1.es.amazonaws.com.cn\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;foo_bot\u0026#34;, \u0026#34;file_types\u0026#34;: \u0026#34;.jpg,.png\u0026#34;, \u0026#34;bot_image\u0026#34;: \u0026#34;foo-bot-img.ecr.amazon.com\u0026#34;, \u0026#34;bot_image_cmd\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;endpoint_name\u0026#34;: \u0026#34;foo-autogluon-sagemaker-inference\u0026#34;, \u0026#34;endpoint_ecr_image_path\u0026#34;: \u0026#34;foo-endpoint.ecr.amazon.com\u0026#34;, \u0026#34;instance_type\u0026#34;: \u0026#34;ml.m5.large\u0026#34;, \u0026#34;model_s3_path\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;create_date\u0026#34;: \u0026#34;2020-07-27 21:39:00\u0026#34;, \u0026#34;update_date\u0026#34;: \u0026#34;2020-07-27 22:39:00\u0026#34; }  es_url从Elasticsearch控制台中查出访问URL name新Bot的名称 file_types待处理文件的后缀，不能匹配的文件将不被处理 bot_imageBot ECR的URL，执行框架将自动下载并启动 bot_image_cmd执行时传给bot ECR的参数（一般不需要） endpoint_name endpoint的名称，如果该Endpoint不存在则会按照下面endpoint_ecr_image_path制定的URL启动endpoint endpoint_ecr_image_pathEndpoint ECR的URL instance_typeEndpoint运行的机型（中国区帐号默认有两台m5.xlarge） model_s3_pathOptional, 配合BYOM模式 create_date可以保持默认值 update_date可以保持默认值  python脚本执行的输出是最后从ES中查找出来的bot配置。\n测试机器人 参考以上章节\n"},{"uri":"/06deepdivebot/060105checkstepfunction.html","title":"检查 StepFunction","tags":[],"description":"","content":"提交 Job 后，系统会启动 StepFunction，然后 Step Function 会启动 Batch。\nStep Function  点击链接 https://cn-northwest-1.console.amazonaws.cn/states/home?region=cn-northwest-1#/statemachines   点击 spot_bot_controller 点击 【执行】 标签页 选择 CHINESE_ID_OCR-1-xxx-xxx-xxx的任务。 显示 【查看详细信息】 页面\n"},{"uri":"/07cleanup/0710sagemaker.html","title":"清理 SageMaker","tags":[],"description":"","content":"使用 APIGateway 进行删除 假设您是在宁夏 region 部署的 spotbot。 请点击 https://cn-northwest-1.console.amazonaws.cn/apigateway/main/apis?region=cn-northwest-1\n点击 Spotbot，进入 API 详情页面。 在 资源 列，选择 ANY, 如下图所示 点击 【测试】链接，方法选择 DELETE，请求正文选择\n{\u0026quot;bot_name\u0026quot;: \u0026quot;SENTIMENT_ANALYSIS\u0026quot;} 点击 【测试】按钮。 资源清理任务开始。 其他机器人的 sagemaker 资源的清理工作，步骤同上。仅仅是需要使用如下的 json字符串在请求正文中请求。\n情感分析机器人 {\u0026quot;bot_name\u0026quot;: \u0026quot;SENTIMENT_ANALYSIS\u0026quot;} 车型分类机器人 {\u0026quot;bot_name\u0026quot;: \u0026quot;CAR_ACCIDENT_INSPECTOR\u0026quot;} 身份证识别机器人 {\u0026quot;bot_name\u0026quot;: \u0026quot;CHINESE_ID_OCR\u0026quot;} 在 SageMaker 中验证资源已经被清理 假设您是在宁夏 region 部署的 spotbot。 请点击 https://cn-northwest-1.console.amazonaws.cn/sagemaker/home?region=cn-northwest-1#/dashboard 点击 【终端节点】 "},{"uri":"/07cleanup/0720batch.html","title":"清理 Batch","tags":[],"description":"","content":"删除 Batch 由于 AWS Batch 是由 Lambda 创建的，所以在调用 Cloudformation 删除操作之前，需要手工的删除 Batch。\n删除作业定义 假设您在宁夏 Region 创建SpotBot，那么请登录 Batch Console.\n 点击 https://cn-northwest-1.console.amazonaws.cn/batch/home?region=cn-northwest-1#/dashboard 选择左侧 “作业定义”  点击 【作业定义】中 CAR_ACCIDENT_INSPECTOR_def 链接  选择 一个作业定义，例如 Rivision 21, 然后点击 【操作】按钮， 选择 【取消注册】  依次选择所有的作业定义，执行重复的操作。删除所有的作业定义。   删除作业队列 1.选择一个作业队列，例如 SENTIMENT_ANALYSIS_queue_spot_bot_compute 首先点击 【禁用】按钮 2.确保该作业队列的状态变为 DISABLED. 3.再选择 选择一个作业队列，例如 SENTIMENT_ANALYSIS_queue_spot_bot_compute 点击【删除】按钮 4.确保当前状态为 DELETING 状态。\n依次选择所有的作业队列，执行重复的操作。删除所有的作业队列。   删除计算环境  选择【计算环境】,例如 Spot-bot-compute  点击 【禁用】按钮 确保状态问 DISABLED 状态。 选择【计算环境】,例如 Spot-bot-compute，点击 【删除】按钮。确保当前状态为 DELETING 状态。  等待 计算环境 被删除。   "},{"uri":"/07cleanup/0730stepfunction.html","title":"清理 StepFunction","tags":[],"description":"","content":"删除 Step Function 由于 AWS Batch 是由 Lambda 创建的，所以在调用 Cloudformation 删除操作之前，需要手工的删除 Batch。\n删除作业定义 假设您在宁夏 Region 创建SpotBot，那么请登录 Step Functions Console. 点击 https://cn-northwest-1.console.amazonaws.cn/states/home?region=cn-northwest-1#/statemachines\n 选择 【状态机】，选择 spot_bot_controller_v1  点击 【删除】按钮。  选择 【删除状态机】按钮。  正在删除。稍等一会儿, Step Function被删除。   "},{"uri":"/07cleanup/0740cleanup.html","title":"清理堆栈 SpotBot","tags":[],"description":"","content":"清理 Spotbot 整个堆栈 清理 Spotbot Bastion 假设您在宁夏 Region 创建SpotBot，那么请登录 Cloudformation Console. https://cn-northwest-1.console.amazonaws.cn/cloudformation/home?region=cn-northwest-1#/ 选择 【堆栈】,在过滤条件中输入 \u0026ldquo;spotbot\u0026rdquo; 作为关键字。如下图所示  选择 \u0026ldquo;spotbot-bastion\u0026rdquo;, 点击 【删除】按钮。  选择 【删除堆栈】确认按钮。  等待大概 2 分钟，spotbot-bastion 堆栈删除。   清理 Spotbot 堆栈  选择 \u0026ldquo;spot-bot\u0026rdquo;, 点击 【删除】按钮。  堆栈开始删除，状态已经更改为 “DELETE_IN_PROCESS”  由于是嵌套堆栈，所以 spot-bot 的堆栈删除的同时，spot-bot-spotbotVPC 也会跟随删除。 由于堆栈中包含 Elasticsearch 资源，该资源删除时间比较长。 等待大概 15 分钟后，所有资源清理干净。  "},{"uri":"/05usebot/0501ocrbot/050101submitjob.html","title":"场景文字识别机器人-提交任务","tags":[],"description":"","content":"测试机器人方法 提交数字化资产盘活任务有两种方法：仅使用一种方法即可\n 使用 curl 在命令行的方式下提交 job。 在 API Gateway console 下提交 job。  适合于没有开通 ICP Excepiton 的场景下使用。    下面就两种方法依次描述。\n使用 curl 在命令行的方式下提交 job 为了方便使用 curl 的方式提交，可以使用在 使用机器人 部分所生成的堡垒机提交任务。\n  记录下堡垒机的 EIP。（具体信息可以查看 Cloudformation 堆栈 spotbot-bastion中【资源】标签页中的 EIP 部分） 点击 【资源】 标签页，记录 EIP。例如下图中的 EIP 字段 52.82.89.118   ssh 方式登录到堡垒机,假设使用的 keypair 是 ningxia.pem。由于在Cloudformation 堆栈 spotbot-bastion 使用的是 Amazon Linux2 AMI, 所以 EC2 用户名是 ec2-user 注意： 请修改 52.82.89.118 为您实际生成堡垒机的 EIP\n  ssh -i ningxia.pem ec2-user@52.82.89.118 SSH 登录成功。 在堡垒机运行如下命令，或者自己 account下的 accountid, apigateway 以及 bucketname  accoutid=$(aws sts get-caller-identity --region cn-northwest-1 --query \u0026#39;Account\u0026#39; --output text) apigatewayid=$(aws apigateway get-rest-apis --query \u0026#34;items[?name==\u0026#39;spot-bot\u0026#39;].id\u0026#34; --region cn-northwest-1 --output text) apigatewayendpoint=\u0026#34;https://$apigatewayid.execute-api.cn-northwest-1.amazonaws.com.cn/Prod/create/proxy\u0026#34; s3bucketname=\u0026#34;spot-bot-exampledata-cn-northwest-1-$accoutid\u0026#34; echo $accoutid echo $apigatewayid echo $apigatewayendpoint echo $s3bucketname 提交job\nocr_bot_json=\u0026#34;{\\\u0026#34;s3_bucket\\\u0026#34;:\\\u0026#34;$s3bucketname\\\u0026#34;, \\\u0026#34;s3_path\\\u0026#34;:\\\u0026#34;ocr_bot/input\\\u0026#34;,\\\u0026#34;bot_name\\\u0026#34;:\\\u0026#34;CHINESE_ID_OCR\\\u0026#34;, \\\u0026#34;number_of_bots\\\u0026#34;:\\\u0026#34;1\\\u0026#34;,\\\u0026#34;bulk_size\\\u0026#34;:\\\u0026#34;500\\\u0026#34;, \\\u0026#34;output_s3_bucket\\\u0026#34;:\\\u0026#34;$s3bucketname\\\u0026#34;,\\\u0026#34;output_s3_prefix\\\u0026#34;:\\\u0026#34;ocr_bot/output\\\u0026#34;}\u0026#34; curl -d \u0026#34;$ocr_bot_json\u0026#34; -X POST $apigatewayendpoint 提交成功后，系统返回jobid\n[ec2-user@ip-10-0-1-246 ~]$ curl -d \u0026#34;$ocr_bot_json\u0026#34; -X POST $apigatewayendpoint {\u0026#34;job_id\u0026#34;: \u0026#34;8f622386-f09b-415b-9a77-c7dcc640d153\u0026#34;} 在 API Gateway console 下提交 job 在 CloudFormation console, 选择堆栈 \u0026ldquo;SpotBotWorkshop\u0026rdquo;, 点击 [Outputs] 标签页, 选择 “SSpotBotCreateApigwURL”, 记录下 APIGateway 的 endpoint. 例如 https://86rp320xzd.execute-api.cn-northwest-1.amazonaws.com/Prod/create/proxyproxy\n进入 APIGateway Console 页面，点击 API \u0026ldquo;Spotbot\u0026rdquo; - 点击左侧的\u0026quot;Stage\u0026rdquo; -\u0026gt; \u0026ldquo;选择 Stages - Prod\u0026rdquo;\n使用 API Gateway 进行测试 进入 API Gateway console 页面，点击 左侧的 [Resources] , 选择 /create/{any_argument+}/ANY 点击 [Test] 链接\n具体信息如下：\n Method 选择 Post 请注意把 s3_bucket 名称 spot-bot-exampledata-cn-northwest-1-123456789012 中的123456789012 替换为自己的 accountid。  请注意把 output_s3_bucket 名称 spot-bot-exampledata-cn-northwest-1-123456789012 中的123456789012 替换为自己的 accountid。   Reqeust Body 填入如下信息：  {\u0026#34;s3_bucket\u0026#34;:\u0026#34;spot-bot-exampledata-cn-northwest-1-123456789012\u0026#34;, \u0026#34;s3_path\u0026#34;:\u0026#34;ocr_bot/input\u0026#34;,\u0026#34;bot_name\u0026#34;:\u0026#34;CHINESE_ID_OCR\u0026#34;, \u0026#34;number_of_bots\u0026#34;:\u0026#34;1\u0026#34;,\u0026#34;bulk_size\u0026#34;:\u0026#34;500\u0026#34;, \u0026#34;output_s3_bucket\u0026#34;:\u0026#34;spot-bot-exampledata-cn-northwest-1-123456789012\u0026#34;,\u0026#34;output_s3_prefix\u0026#34;:\u0026#34;ocr_bot/output\u0026#34;} 填好相关字段，如下图所示 点击[Test] 按钮提交任务。\n查看右侧的 log 区域，任务成功提交。 "},{"uri":"/05usebot/0501ocrbot/050102checkresult.html","title":"场景文字识别机器人-测试结果","tags":[],"description":"","content":"提交任务后，需要到 Batch console 中检查作业运行状态。 点击 https://cn-northwest-1.console.amazonaws.cn/batch/home?region=cn-northwest-1#/dashboard\n在状态区域选择 \u0026ldquo;succeeded”。\n如果状态已经是 SUCCEEDED 状态，说明 Job 已经完成。\n 进入 S3 Console,下载执行结果。 点击 https://console.amazonaws.cn/s3/home?region=cn-northwest-1 选择 spot-bot-exampledata-cn-northwest-1-123456789012 (注： 123456789012需替换为你实际的 accoutid) 进入 /ocr_bot/out_put folder 下载推理结果文件。  测试样例及结果 输入文件 ocr_3.png 内容为\n原始文件 推理结果：\n{\u0026#34;s3://spot-bot-exampledata-cn-northwest-1-123456789012/ocr_bot/input/ocr_3.png\u0026#34;: \u0026#34;有关电波接收的要点,L募财谦纯熟饿录艇腰嫌\u0026#34;} "},{"uri":"/05usebot/0502carbot/050201submitjob.html","title":"车型识别机器人-提交任务","tags":[],"description":"","content":"测试机器人方法 提交数字化资产盘活任务有两种方法：仅使用一种方法即可\n 使用 curl 在命令行的方式下提交 job。 在 API Gateway console 下提交 job。  适合于没有开通 ICP Excepiton 的场景下使用。    使用 curl 在命令行的方式下提交 job 为了方便使用 curl 的方式提交，可以使用在 使用机器人 部分所生成的堡垒机提交任务。\n  记录下堡垒机的 EIP。（具体信息可以查看 Cloudformation 堆栈 spotbot-bastion中【资源】标签页中的 EIP 部分） 点击 【资源】 标签页，记录 EIP。例如下图中的 EIP 字段 52.82.89.118   ssh 方式登录到堡垒机,假设使用的 keypair 是 ningxia.pem。由于在Cloudformation 堆栈 spotbot-bastion 使用的是 Amazon Linux2 AMI, 所以 EC2 用户名是 ec2-user 注意： 请修改 52.82.89.118 为您实际生成堡垒机的 EIP\n  ssh -i ningxia.pem ec2-user@52.82.89.118 SSH 登录成功。 在堡垒机运行如下命令，或者自己 account下的 accountid, apigateway 以及 bucketname  accoutid=$(aws sts get-caller-identity --region cn-northwest-1 --query \u0026#39;Account\u0026#39; --output text) apigatewayid=$(aws apigateway get-rest-apis --query \u0026#34;items[?name==\u0026#39;spot-bot\u0026#39;].id\u0026#34; --region cn-northwest-1 --output text) apigatewayendpoint=\u0026#34;https://$apigatewayid.execute-api.cn-northwest-1.amazonaws.com.cn/Prod/create/proxy\u0026#34; s3bucketname=\u0026#34;spot-bot-exampledata-cn-northwest-1-$accoutid\u0026#34; echo $accoutid echo $apigatewayid echo $apigatewayendpoint echo $s3bucketname 提交job\ncar_bot_json=\u0026#34;{\\\u0026#34;s3_bucket\\\u0026#34;:\\\u0026#34;$s3bucketname\\\u0026#34;, \\\u0026#34;s3_path\\\u0026#34;:\\\u0026#34;car_bot/input\\\u0026#34;,\\\u0026#34;bot_name\\\u0026#34;:\\\u0026#34;CAR_ACCIDENT_INSPECTOR\\\u0026#34;, \\\u0026#34;number_of_bots\\\u0026#34;:\\\u0026#34;1\\\u0026#34;,\\\u0026#34;bulk_size\\\u0026#34;:\\\u0026#34;500\\\u0026#34;, \\\u0026#34;output_s3_bucket\\\u0026#34;:\\\u0026#34;$s3bucketname\\\u0026#34;,\\\u0026#34;output_s3_prefix\\\u0026#34;:\\\u0026#34;car_bot/output\\\u0026#34;}\u0026#34; curl -d \u0026#34;$car_bot_json\u0026#34; -X POST $apigatewayendpoint 提交成功后，系统返回jobid\n[ec2-user@ip-10-0-1-246 ~]$ curl -d \u0026#34;$car_bot_json\u0026#34; -X POST $apigatewayendpoint {\u0026#34;job_id\u0026#34;: \u0026#34;8f622386-f09b-415b-9a77-c7dcc640d153\u0026#34;} 在 API Gateway console 下提交 job 进入 API Gateway console 页面，点击 左侧的 [Resources] , 选择 /create/{any_argument+}/ANY 点击 [Test] 链接\n具体信息如下：\n Method 选择 Post 请注意把 s3_bucket 名称 spot-bot-exampledata-cn-northwest-1-123456789012 中的123456789012 替换为自己的 accountid。  请注意把 output_s3_bucket 名称 spot-bot-exampledata-cn-northwest-1-123456789012 中的123456789012 替换为自己的 accountid。   Reqeust Body 填入如下信息：  {\u0026#34;s3_bucket\u0026#34;:\u0026#34;spot-bot-exampledata-cn-northwest-1-123456789012\u0026#34;, \u0026#34;s3_path\u0026#34;:\u0026#34;ocr_bot/input\u0026#34;,\u0026#34;bot_name\u0026#34;:\u0026#34;CAR_ACCIDENT_INSPECTOR\u0026#34;, \u0026#34;number_of_bots\u0026#34;:\u0026#34;1\u0026#34;,\u0026#34;bulk_size\u0026#34;:\u0026#34;500\u0026#34;, \u0026#34;output_s3_bucket\u0026#34;:\u0026#34;spot-bot-exampledata-cn-northwest-1-123456789012\u0026#34;,\u0026#34;output_s3_prefix\u0026#34;:\u0026#34;ocr_bot/output\u0026#34;} "},{"uri":"/05usebot/0502carbot/050202checkresult.html","title":"车型识别机器人-测试结果","tags":[],"description":"","content":"提交任务后，需要到 Batch console 中检查作业运行状态。 点击 https://cn-northwest-1.console.amazonaws.cn/batch/home?region=cn-northwest-1#/dashboard\n在状态区域选择 \u0026ldquo;succeeded”。\n如果状态已经是 SUCCEEDED 状态，说明 Job 已经完成。\n 进入 S3 Console,下载执行结果。 点击 https://console.amazonaws.cn/s3/home?region=cn-northwest-1 选择 spot-bot-exampledata-cn-northwest-1-123456789012 (注： 123456789012需替换为你实际的 accoutid) 进入 /car_bot/out_put folder 下载推理结果文件。  测试样例及结果 输入文件 2020-Audi-TTS-Coupe-Orange.png 图片显示的是 Audi TT 输出结果 2020-Audi-TTS-Coupe-Orange.json\n注意：由于程序的 bug,实际在桶里生成的文件名是 2020-Audi-TTS-Coupe-Orange.png，请大家下载后自行更改文件扩展名为 2020-Audi-TTS-Coupe-Orange.json\n(注： 123456789012需替换为你实际的 accoutid)\n{\u0026#34;s3://spot-bot-exampledata-cn-northwest-1-123456789012/car_bot/input/2020-Audi-TTS-Coupe-Orange.png\u0026#34;: \u0026#34;{\\\u0026#34;Output\\\u0026#34;: {\\\u0026#34;TypeId\\\u0026#34;: 23, \\\u0026#34;TypeDescription\\\u0026#34;: \\\u0026#34;Audi_TT_RS_Coupe_2012\\\u0026#34;, \\\u0026#34;Maker\\\u0026#34;: \\\u0026#34;Audi\\\u0026#34;, \\\u0026#34;Model\\\u0026#34;: \\\u0026#34;TT RS Coupe\\\u0026#34;, \\\u0026#34;Year\\\u0026#34;: \\\u0026#34;2012\\\u0026#34;}}\u0026#34;} "},{"uri":"/05usebot/0503semanticbot/050301submitjob.html","title":"情感识别机器人-提交任务","tags":[],"description":"","content":"测试机器人方法 提交数字化资产盘活任务有两种方法：仅使用一种方法即可\n 使用 curl 在命令行的方式下提交 job。 在 API Gateway console 下提交 job。  适合于没有开通 ICP Excepiton 的场景下使用。    使用 curl 在命令行的方式下提交 job 为了方便使用 curl 的方式提交，可以使用在 使用机器人 部分所生成的堡垒机提交任务。\n  记录下堡垒机的 EIP。（具体信息可以查看 Cloudformation 堆栈 spotbot-bastion中【资源】标签页中的 EIP 部分） 点击 【资源】 标签页，记录 EIP。例如下图中的 EIP 字段 52.82.89.118   ssh 方式登录到堡垒机,假设使用的 keypair 是 ningxia.pem。由于在Cloudformation 堆栈 spotbot-bastion 使用的是 Amazon Linux2 AMI, 所以 EC2 用户名是 ec2-user 注意： 请修改 52.82.89.118 为您实际生成堡垒机的 EIP\n  ssh -i ningxia.pem ec2-user@52.82.89.118 SSH 登录成功。 在堡垒机运行如下命令，或者自己 account下的 accountid, apigateway 以及 bucketname  accoutid=$(aws sts get-caller-identity --region cn-northwest-1 --query \u0026#39;Account\u0026#39; --output text) apigatewayid=$(aws apigateway get-rest-apis --query \u0026#34;items[?name==\u0026#39;spot-bot\u0026#39;].id\u0026#34; --region cn-northwest-1 --output text) apigatewayendpoint=\u0026#34;https://$apigatewayid.execute-api.cn-northwest-1.amazonaws.com.cn/Prod/create/proxy\u0026#34; s3bucketname=\u0026#34;spot-bot-exampledata-cn-northwest-1-$accoutid\u0026#34; echo $accoutid echo $apigatewayid echo $apigatewayendpoint echo $s3bucketname 在堡垒机运行如下的命令，提交job\nsentiment_bot_json=\u0026#34;{\\\u0026#34;s3_bucket\\\u0026#34;:\\\u0026#34;$s3bucketname\\\u0026#34;, \\\u0026#34;s3_path\\\u0026#34;:\\\u0026#34;sentiment_bot/input\\\u0026#34;,\\\u0026#34;bot_name\\\u0026#34;:\\\u0026#34;SENTIMENT_ANALYSIS\\\u0026#34;, \\\u0026#34;number_of_bots\\\u0026#34;:\\\u0026#34;1\\\u0026#34;,\\\u0026#34;bulk_size\\\u0026#34;:\\\u0026#34;500\\\u0026#34;, \\\u0026#34;output_s3_bucket\\\u0026#34;:\\\u0026#34;$s3bucketname\\\u0026#34;,\\\u0026#34;output_s3_prefix\\\u0026#34;:\\\u0026#34;sentiment_bot/output\\\u0026#34;}\u0026#34; curl -d \u0026#34;$sentiment_bot_json\u0026#34; -X POST $apigatewayendpoint 提交成功后，系统返回jobid\n[ec2-user@ip-10-0-1-246 ~]$ curl -d \u0026#34;$sentiment_bot_json\u0026#34; -X POST $apigatewayendpoint {\u0026#34;job_id\u0026#34;: \u0026#34;8f622386-f09b-415b-9a77-c7dcc640d153\u0026#34;} 在 API Gateway console 下提交 job 进入 API Gateway console 页面，点击 左侧的 [Resources] , 选择 /create/{any_argument+}/ANY 点击 [Test] 链接\n具体信息如下：\n Method 选择 Post 请注意把 s3_bucket 名称 spot-bot-exampledata-cn-northwest-1-123456789012 中的123456789012 替换为自己的 accountid。  请注意把 output_s3_bucket 名称 spot-bot-exampledata-cn-northwest-1-123456789012 中的123456789012 替换为自己的 accountid。   Reqeust Body 填入如下信息：  {\u0026#34;s3_bucket\u0026#34;:\u0026#34;spot-bot-exampledata-cn-northwest-1-123456789012\u0026#34;, \u0026#34;s3_path\u0026#34;:\u0026#34;sentiment_bot/input\u0026#34;,\u0026#34;bot_name\u0026#34;:\u0026#34;SENTIMENT_ANALYSIS\u0026#34;, \u0026#34;number_of_bots\u0026#34;:\u0026#34;1\u0026#34;,\u0026#34;bulk_size\u0026#34;:\u0026#34;500\u0026#34;, \u0026#34;output_s3_bucket\u0026#34;:\u0026#34;spot-bot-exampledata-cn-northwest-1-123456789012\u0026#34;,\u0026#34;output_s3_prefix\u0026#34;:\u0026#34;sentiment_bot/output\u0026#34;} "},{"uri":"/05usebot/0503semanticbot/050302checkresult.html","title":"情感识别机器人-测试结果","tags":[],"description":"","content":"提交任务后，需要到 Batch console 中检查作业运行状态。 点击 https://cn-northwest-1.console.amazonaws.cn/batch/home?region=cn-northwest-1#/dashboard\n在状态区域选择 \u0026ldquo;succeeded”。\n如果状态已经是 SUCCEEDED 状态，说明 Job 已经完成。\n 进入 S3 Console,下载执行结果。 点击 https://console.amazonaws.cn/s3/home?region=cn-northwest-1 选择 spot-bot-exampledata-cn-northwest-1-123456789012 (注： 123456789012需替换为你实际的 accoutid) 进入 /sentiment_bot/out_put folder 下载推理结果文件。  测试样例及结果 输入文件 example_n_1.txt 内容为\n 喝起来就是糖精+维他柠檬茶啊！严重怀疑茶底是用维他柠檬茶加糖精调好的，所以无法调糖，甜到爆炸，喝了几口受不了直接丢了 出品很一般……希望不要因为控制成本而影响出品，这样得不偿失\n 输出结果 example_n_1.json\n(注： 123456789012需替换为你实际的 accoutid)\n{\u0026#34;s3://spot-bot-exampledata-cn-northwest-1-1234567890/sentiment_bot/input/example_n_1.txt\u0026#34;: \u0026#34;negative\u0026#34;} 输入文件 example_p_2.txt 内容为\n 这个打印机还是很不错的。大小很好，适合家庭没什么地方放打印机的。而且打印的速度特别快，基本上我在打印之后不到半分钟吧，内容就已经可以打出来了，很快的，很适合家里有小朋友给孩子打作业呀习题呀。用了几天了，目前各方面都很满意，希望可以像宣传的一样，一盒墨可以打很多吧。\n 输出结果 example_n_1.json\n(注： 123456789012需替换为你实际的 accoutid)\n{\u0026#34;s3://spot-bot-exampledata-cn-northwest-1-1234567890/sentiment_bot/input/example_p_2.txt\u0026#34;: \u0026#34;positive\u0026#34;} "},{"uri":"/06deepdivebot/0601010batch.html","title":"检查 AWS Batch ","tags":[],"description":"","content":"查看 Batch状态 进入 https://cn-northwest-1.console.amazonaws.cn/batch/home?region=cn-northwest-1#\nBatch 作业 Batch 作业定义 点击 【作业定义】，选择 CHINESE_ID_OCR_def 从上图可以看出，该作业定义其实是从 ECR 753680513547.dkr.ecr.cn-northwest-1.amazonaws.com.cn/id_ocr_bot:latest 获得 Docker Image 另外该作业定义使用4 VCPU 和 8096MiB 内存 点击 “Revision26”, 显示详情 "},{"uri":"/categories.html","title":"Categories","tags":[],"description":"","content":""},{"uri":"/","title":"Spot Tagging Bot Workshop","tags":[],"description":"","content":"欢迎使用资产盘活机器人解决方案 概述 如今，许多客户收集了大量的数字资产。例如，保险客户在处理案件时收集复印处方或电子收据。客户必须投入不同的工作人员，以不同的格式阅读资产，如接收、审计、重新审查等，以不同的语言。由于缺乏数字标记/记录，他们无法重复使用资产中的常见见解进行业务预测。在资本市场，客户希望从年度报告中提取财务标签。在银行业，客户正在寻找与客户交谈的柜员标签，以提高质量和管理风险。一些客户设置了一些 ML 服务来从原始数字资产中提取语义标签。算法工程师、数据科学家以及专用计算和存储，这对 客户来说都是宝贵的资源。\n现在，借助资产盘活机器人解决方案，客户可以利用 AWS 功能运行数字资产标记任务。通过利用 S3 和竞价型实例(Spot Instance)，客户可以使用批处理机器人来标记资产。例如，客户可以使用证照识别机器人从旧版身份证复印件中读取用户个人信息。信息将安全地存储在客户的私有 S3 存储桶中，并且可以进一步录入结构化进行二次使用。机器人将在竞价型实例中工作以节省成本。AWS 中国创新解决方案中心将不断发布新的机器人来支持客户需求。第一批机器人将包括基于图像识别的汽车保险理赔机器人，基于ocr的证照识别机器人和基于自然语言处理的差评识别机器人。第二批将包括与视频相关的内容，如配乐文本、目标角色识别、监控对象检测等。第三批将包括语言部分，如关键词检测、年度报告阅读等。\n作为一个开源框架，构建者也可以通过自己的模型或机器人为框架做出贡献。与传统的 AI 管理服务相比，AWS 资产盘活机器人解决方案在客户账户中提供 “内部” 服务。客户可以训练自己的模型，拥有更大的控制力和灵活性，以便以较低的成本执行数据资产标记工作。\n本次 workshop 前提  本次 workshop 建议在 宁夏 Region 使用。为了演示方便，所以本 workshop 所有的演示都会以宁夏 Region 为例。 确保你的 account 已经开ICP Exception，否则无法再从 Internet 访问 APIGateway 以及 EC2 80 及 443 端口。 在宁夏 Region 提前生成一个 KeyPair 用于堡垒机的登录与测试。  本次 workshop 目的  让大家对于数字化资产盘活机器人的功能以及架构有所了解。 能够让大家在自己的 AWS Account 部署成功资产盘活机器人解决方案。 能够完成一个 BYOB 实验。  "},{"uri":"/tags.html","title":"Tags","tags":[],"description":"","content":""}]